{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "import string\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction import text \n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "spacy.prefer_gpu()\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import re\n",
    "import string\n",
    "import funcy as fp\n",
    "from gensim import models\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_pickle('lemmatized_bigstrings.pickle')\n",
    "# data_df['index'] = data_df.index\n",
    "# data_df = data_df.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>wsb</th>\n",
       "      <td>future apple   share gran kid ask not buy appl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text\n",
       "wsb  future apple   share gran kid ask not buy appl..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>wsb</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0\n",
       "wsb  1.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvn = TfidfVectorizer(ngram_range=(1,2))\n",
    "data_cvn = cvn.fit_transform(data_df[:1].T)\n",
    "data_dtmn = pd.DataFrame(data_cvn.toarray(), columns=cvn.get_feature_names()).T\n",
    "# data_dtmn.columns = data_df.index\n",
    "data_dtmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusn = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmn))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordn = dict((v, k) for k, v in cvn.vocabulary_.items())\n",
    "word2id = dict((k, v) for k, v in cvn.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 22:41:18: using symmetric alpha at 0.1\n",
      "INFO - 22:41:18: using symmetric eta at 0.1\n",
      "INFO - 22:41:18: using serial LDA version on this node\n",
      "INFO - 22:41:18: running online LDA training, 10 topics, 50 passes over the supplied corpus of 1 documents, updating every 30000 documents, evaluating every ~1 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "INFO - 22:41:18: training LDA model using 15 processes\n",
      "INFO - 22:41:18: PROGRESS: pass 0, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "INFO - 22:41:18: topic #4 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #1 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #7 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #2 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #0 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic diff=0.000000, rho=1.000000\n",
      "INFO - 22:41:18: -2.302 per-word bound, 4.9 perplexity estimate based on a held-out corpus of 1 documents with 1 words\n",
      "INFO - 22:41:18: PROGRESS: pass 1, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "INFO - 22:41:18: topic #1 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #7 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #6 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #3 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #9 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic diff=0.000000, rho=0.707018\n",
      "INFO - 22:41:18: -2.302 per-word bound, 4.9 perplexity estimate based on a held-out corpus of 1 documents with 1 words\n",
      "INFO - 22:41:18: PROGRESS: pass 2, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "INFO - 22:41:18: topic #6 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #0 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #9 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #5 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #1 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic diff=0.000000, rho=0.577302\n",
      "INFO - 22:41:18: -2.302 per-word bound, 4.9 perplexity estimate based on a held-out corpus of 1 documents with 1 words\n",
      "INFO - 22:41:18: PROGRESS: pass 3, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "INFO - 22:41:18: topic #3 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #2 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #4 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #1 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #5 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic diff=0.000000, rho=0.499969\n",
      "INFO - 22:41:18: -2.302 per-word bound, 4.9 perplexity estimate based on a held-out corpus of 1 documents with 1 words\n",
      "INFO - 22:41:18: PROGRESS: pass 4, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "INFO - 22:41:18: topic #5 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #3 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #8 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #0 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #2 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic diff=0.000000, rho=0.447191\n",
      "INFO - 22:41:18: -2.302 per-word bound, 4.9 perplexity estimate based on a held-out corpus of 1 documents with 1 words\n",
      "INFO - 22:41:18: PROGRESS: pass 5, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "INFO - 22:41:18: topic #9 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #1 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #3 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #2 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #0 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic diff=0.000000, rho=0.408231\n",
      "INFO - 22:41:18: -2.302 per-word bound, 4.9 perplexity estimate based on a held-out corpus of 1 documents with 1 words\n",
      "INFO - 22:41:18: PROGRESS: pass 6, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "INFO - 22:41:18: topic #8 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #3 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #6 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #1 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #9 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic diff=0.000000, rho=0.377951\n",
      "INFO - 22:41:18: -2.302 per-word bound, 4.9 perplexity estimate based on a held-out corpus of 1 documents with 1 words\n",
      "INFO - 22:41:18: PROGRESS: pass 7, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "INFO - 22:41:18: topic #0 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #9 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #1 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #5 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #4 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic diff=0.000000, rho=0.353542\n",
      "INFO - 22:41:18: -2.302 per-word bound, 4.9 perplexity estimate based on a held-out corpus of 1 documents with 1 words\n",
      "INFO - 22:41:18: PROGRESS: pass 8, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "INFO - 22:41:18: topic #7 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #1 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #8 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #4 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #5 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic diff=0.000000, rho=0.333324\n",
      "INFO - 22:41:18: -2.302 per-word bound, 4.9 perplexity estimate based on a held-out corpus of 1 documents with 1 words\n",
      "INFO - 22:41:18: PROGRESS: pass 9, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "INFO - 22:41:18: topic #8 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #0 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #9 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #7 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #4 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic diff=0.000000, rho=0.316220\n",
      "INFO - 22:41:18: -2.302 per-word bound, 4.9 perplexity estimate based on a held-out corpus of 1 documents with 1 words\n",
      "INFO - 22:41:18: PROGRESS: pass 10, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "INFO - 22:41:18: topic #6 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #4 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #9 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #5 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #3 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic diff=0.000000, rho=0.301504\n",
      "INFO - 22:41:18: -2.302 per-word bound, 4.9 perplexity estimate based on a held-out corpus of 1 documents with 1 words\n",
      "INFO - 22:41:18: PROGRESS: pass 11, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "INFO - 22:41:18: topic #7 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #9 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #2 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #4 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #8 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic diff=0.000000, rho=0.288669\n",
      "INFO - 22:41:18: -2.302 per-word bound, 4.9 perplexity estimate based on a held-out corpus of 1 documents with 1 words\n",
      "INFO - 22:41:18: PROGRESS: pass 12, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "INFO - 22:41:18: topic #7 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #0 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #8 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #3 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #1 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic diff=0.000000, rho=0.277345\n",
      "INFO - 22:41:18: -2.302 per-word bound, 4.9 perplexity estimate based on a held-out corpus of 1 documents with 1 words\n",
      "INFO - 22:41:18: PROGRESS: pass 13, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "INFO - 22:41:18: topic #9 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #6 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #7 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #5 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #0 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic diff=0.000000, rho=0.267256\n",
      "INFO - 22:41:18: -2.302 per-word bound, 4.9 perplexity estimate based on a held-out corpus of 1 documents with 1 words\n",
      "INFO - 22:41:18: PROGRESS: pass 14, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "INFO - 22:41:18: topic #2 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #1 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #8 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #4 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #0 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic diff=0.000000, rho=0.258195\n",
      "INFO - 22:41:18: -2.302 per-word bound, 4.9 perplexity estimate based on a held-out corpus of 1 documents with 1 words\n",
      "INFO - 22:41:18: PROGRESS: pass 15, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "INFO - 22:41:18: topic #5 (0.100): 1.000*\"wsb\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 22:41:18: topic #9 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #0 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #7 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #3 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic diff=0.000000, rho=0.249996\n",
      "INFO - 22:41:18: -2.302 per-word bound, 4.9 perplexity estimate based on a held-out corpus of 1 documents with 1 words\n",
      "INFO - 22:41:18: PROGRESS: pass 16, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "INFO - 22:41:18: topic #8 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #1 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #9 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #5 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #6 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic diff=0.000000, rho=0.242532\n",
      "INFO - 22:41:18: -2.302 per-word bound, 4.9 perplexity estimate based on a held-out corpus of 1 documents with 1 words\n",
      "INFO - 22:41:18: PROGRESS: pass 17, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "INFO - 22:41:18: topic #6 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #4 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #8 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #3 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #9 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic diff=0.000000, rho=0.235699\n",
      "INFO - 22:41:18: -2.302 per-word bound, 4.9 perplexity estimate based on a held-out corpus of 1 documents with 1 words\n",
      "INFO - 22:41:18: PROGRESS: pass 18, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "INFO - 22:41:18: topic #1 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #5 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #8 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #2 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #9 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic diff=0.000000, rho=0.229413\n",
      "INFO - 22:41:18: -2.302 per-word bound, 4.9 perplexity estimate based on a held-out corpus of 1 documents with 1 words\n",
      "INFO - 22:41:18: PROGRESS: pass 19, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "INFO - 22:41:18: topic #3 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #4 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #2 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #6 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #5 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic diff=0.000000, rho=0.223604\n",
      "INFO - 22:41:18: -2.302 per-word bound, 4.9 perplexity estimate based on a held-out corpus of 1 documents with 1 words\n",
      "INFO - 22:41:18: PROGRESS: pass 20, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "INFO - 22:41:18: topic #1 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #4 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #3 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #0 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #2 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic diff=0.000000, rho=0.218215\n",
      "INFO - 22:41:18: -2.302 per-word bound, 4.9 perplexity estimate based on a held-out corpus of 1 documents with 1 words\n",
      "INFO - 22:41:18: PROGRESS: pass 21, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "INFO - 22:41:18: topic #5 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #8 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #7 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #3 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #9 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic diff=0.000000, rho=0.213198\n",
      "INFO - 22:41:18: -2.302 per-word bound, 4.9 perplexity estimate based on a held-out corpus of 1 documents with 1 words\n",
      "INFO - 22:41:18: PROGRESS: pass 22, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "INFO - 22:41:18: topic #3 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #6 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #4 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #7 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #0 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic diff=0.000000, rho=0.208512\n",
      "INFO - 22:41:18: -2.302 per-word bound, 4.9 perplexity estimate based on a held-out corpus of 1 documents with 1 words\n",
      "INFO - 22:41:18: PROGRESS: pass 23, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "INFO - 22:41:18: topic #4 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #0 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #7 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #2 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #6 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic diff=0.000000, rho=0.204122\n",
      "INFO - 22:41:18: -2.302 per-word bound, 4.9 perplexity estimate based on a held-out corpus of 1 documents with 1 words\n",
      "INFO - 22:41:18: PROGRESS: pass 24, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "INFO - 22:41:18: topic #6 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #3 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #5 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #0 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #7 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic diff=0.000000, rho=0.199998\n",
      "INFO - 22:41:18: -2.302 per-word bound, 4.9 perplexity estimate based on a held-out corpus of 1 documents with 1 words\n",
      "INFO - 22:41:18: PROGRESS: pass 25, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "INFO - 22:41:18: topic #6 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #5 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #2 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #7 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #0 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic diff=0.000000, rho=0.196114\n",
      "INFO - 22:41:18: -2.302 per-word bound, 4.9 perplexity estimate based on a held-out corpus of 1 documents with 1 words\n",
      "INFO - 22:41:18: PROGRESS: pass 26, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "INFO - 22:41:18: topic #7 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #0 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #1 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #6 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #9 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic diff=0.000000, rho=0.192448\n",
      "INFO - 22:41:18: -2.302 per-word bound, 4.9 perplexity estimate based on a held-out corpus of 1 documents with 1 words\n",
      "INFO - 22:41:18: PROGRESS: pass 27, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "INFO - 22:41:18: topic #1 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #2 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #9 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #6 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #4 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic diff=0.000000, rho=0.188981\n",
      "INFO - 22:41:18: -2.302 per-word bound, 4.9 perplexity estimate based on a held-out corpus of 1 documents with 1 words\n",
      "INFO - 22:41:18: PROGRESS: pass 28, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "INFO - 22:41:18: topic #7 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #6 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #4 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #3 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #2 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic diff=0.000000, rho=0.185694\n",
      "INFO - 22:41:18: -2.302 per-word bound, 4.9 perplexity estimate based on a held-out corpus of 1 documents with 1 words\n",
      "INFO - 22:41:18: PROGRESS: pass 29, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "INFO - 22:41:18: topic #9 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #3 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #5 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #8 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #6 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic diff=0.000000, rho=0.182573\n",
      "INFO - 22:41:18: -3.015 per-word bound, 8.1 perplexity estimate based on a held-out corpus of 1 documents with 1 words\n",
      "INFO - 22:41:18: PROGRESS: pass 30, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "INFO - 22:41:18: topic #5 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #2 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #9 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #6 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #1 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic diff=0.000000, rho=0.179604\n",
      "INFO - 22:41:18: -2.302 per-word bound, 4.9 perplexity estimate based on a held-out corpus of 1 documents with 1 words\n",
      "INFO - 22:41:18: PROGRESS: pass 31, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "INFO - 22:41:18: topic #1 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #8 (0.100): 1.000*\"wsb\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 22:41:18: topic #2 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #9 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #4 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic diff=0.000000, rho=0.176775\n",
      "INFO - 22:41:18: -2.302 per-word bound, 4.9 perplexity estimate based on a held-out corpus of 1 documents with 1 words\n",
      "INFO - 22:41:18: PROGRESS: pass 32, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "INFO - 22:41:18: topic #2 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #9 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #6 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #3 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #0 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic diff=0.000000, rho=0.174076\n",
      "INFO - 22:41:18: -2.302 per-word bound, 4.9 perplexity estimate based on a held-out corpus of 1 documents with 1 words\n",
      "INFO - 22:41:18: PROGRESS: pass 33, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "INFO - 22:41:18: topic #3 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #7 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #9 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #5 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #0 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic diff=0.000000, rho=0.171497\n",
      "INFO - 22:41:18: -2.302 per-word bound, 4.9 perplexity estimate based on a held-out corpus of 1 documents with 1 words\n",
      "INFO - 22:41:18: PROGRESS: pass 34, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "INFO - 22:41:18: topic #9 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #0 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #6 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #7 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #3 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic diff=0.000000, rho=0.169030\n",
      "INFO - 22:41:18: -2.302 per-word bound, 4.9 perplexity estimate based on a held-out corpus of 1 documents with 1 words\n",
      "INFO - 22:41:18: PROGRESS: pass 35, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "INFO - 22:41:18: topic #6 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #4 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #9 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #1 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #3 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic diff=0.000000, rho=0.166666\n",
      "INFO - 22:41:18: -2.302 per-word bound, 4.9 perplexity estimate based on a held-out corpus of 1 documents with 1 words\n",
      "INFO - 22:41:18: PROGRESS: pass 36, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "INFO - 22:41:18: topic #7 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #1 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #0 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #8 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #2 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic diff=0.000000, rho=0.164398\n",
      "INFO - 22:41:18: -2.302 per-word bound, 4.9 perplexity estimate based on a held-out corpus of 1 documents with 1 words\n",
      "INFO - 22:41:18: PROGRESS: pass 37, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "INFO - 22:41:18: topic #8 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #3 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #7 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #4 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #5 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic diff=0.000000, rho=0.162220\n",
      "INFO - 22:41:18: -2.302 per-word bound, 4.9 perplexity estimate based on a held-out corpus of 1 documents with 1 words\n",
      "INFO - 22:41:18: PROGRESS: pass 38, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "INFO - 22:41:18: PROGRESS: pass 39, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "INFO - 22:41:18: topic #0 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #7 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #4 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #5 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #1 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic diff=0.000000, rho=0.158113\n",
      "INFO - 22:41:18: -2.302 per-word bound, 4.9 perplexity estimate based on a held-out corpus of 1 documents with 1 words\n",
      "INFO - 22:41:18: PROGRESS: pass 40, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "INFO - 22:41:18: topic #5 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #0 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #7 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #4 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #1 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic diff=0.000000, rho=0.156173\n",
      "INFO - 22:41:18: -2.302 per-word bound, 4.9 perplexity estimate based on a held-out corpus of 1 documents with 1 words\n",
      "INFO - 22:41:18: PROGRESS: pass 41, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "INFO - 22:41:18: topic #2 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #7 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #1 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #9 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #3 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic diff=0.000000, rho=0.154302\n",
      "INFO - 22:41:18: -2.302 per-word bound, 4.9 perplexity estimate based on a held-out corpus of 1 documents with 1 words\n",
      "INFO - 22:41:18: PROGRESS: pass 42, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "INFO - 22:41:18: topic #6 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #7 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #9 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #3 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #4 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic diff=0.000000, rho=0.152498\n",
      "INFO - 22:41:18: -2.302 per-word bound, 4.9 perplexity estimate based on a held-out corpus of 1 documents with 1 words\n",
      "INFO - 22:41:18: PROGRESS: pass 43, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "INFO - 22:41:18: topic #9 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #1 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #6 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #4 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #0 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic diff=0.000000, rho=0.150755\n",
      "INFO - 22:41:18: -2.302 per-word bound, 4.9 perplexity estimate based on a held-out corpus of 1 documents with 1 words\n",
      "INFO - 22:41:18: PROGRESS: pass 44, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "INFO - 22:41:18: topic #9 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #6 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #3 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #4 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #5 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic diff=0.000000, rho=0.149070\n",
      "INFO - 22:41:18: -2.302 per-word bound, 4.9 perplexity estimate based on a held-out corpus of 1 documents with 1 words\n",
      "INFO - 22:41:18: PROGRESS: pass 45, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "INFO - 22:41:18: topic #0 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #8 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #5 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #2 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #3 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic diff=0.000000, rho=0.147441\n",
      "INFO - 22:41:18: -2.302 per-word bound, 4.9 perplexity estimate based on a held-out corpus of 1 documents with 1 words\n",
      "INFO - 22:41:18: PROGRESS: pass 46, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "INFO - 22:41:18: topic #1 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #5 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #9 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #4 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #7 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic diff=0.000000, rho=0.145864\n",
      "INFO - 22:41:18: -2.302 per-word bound, 4.9 perplexity estimate based on a held-out corpus of 1 documents with 1 words\n",
      "INFO - 22:41:18: PROGRESS: pass 47, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "INFO - 22:41:18: topic #3 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #0 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #4 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #1 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #9 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic diff=0.000000, rho=0.144337\n",
      "INFO - 22:41:18: -2.302 per-word bound, 4.9 perplexity estimate based on a held-out corpus of 1 documents with 1 words\n",
      "INFO - 22:41:18: PROGRESS: pass 48, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "INFO - 22:41:18: topic #5 (0.100): 1.000*\"wsb\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 22:41:18: topic #7 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #1 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #3 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #9 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic diff=0.000000, rho=0.142856\n",
      "INFO - 22:41:18: -2.302 per-word bound, 4.9 perplexity estimate based on a held-out corpus of 1 documents with 1 words\n",
      "INFO - 22:41:18: PROGRESS: pass 49, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "INFO - 22:41:18: topic #3 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #6 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #4 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #2 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #9 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic diff=0.000000, rho=0.141421\n",
      "INFO - 22:41:18: -2.302 per-word bound, 4.9 perplexity estimate based on a held-out corpus of 1 documents with 1 words\n",
      "INFO - 22:41:18: topic #0 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #1 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #2 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #3 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #4 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #5 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #6 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #7 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #8 (0.100): 1.000*\"wsb\"\n",
      "INFO - 22:41:18: topic #9 (0.100): 1.000*\"wsb\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, '1.000*\"wsb\"'),\n",
       " (1, '1.000*\"wsb\"'),\n",
       " (2, '1.000*\"wsb\"'),\n",
       " (3, '1.000*\"wsb\"'),\n",
       " (4, '1.000*\"wsb\"'),\n",
       " (5, '1.000*\"wsb\"'),\n",
       " (6, '1.000*\"wsb\"'),\n",
       " (7, '1.000*\"wsb\"'),\n",
       " (8, '1.000*\"wsb\"'),\n",
       " (9, '1.000*\"wsb\"')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with 2 topics\n",
    "ldan = models.LdaMulticore(corpus=corpusn, num_topics=10, id2word=id2wordn, passes=50)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 22:41:19: saving LdaState object under all_subs.model.state, separately None\n",
      "INFO - 22:41:19: saved all_subs.model.state\n",
      "INFO - 22:41:19: saving LdaMulticore object under all_subs.model, separately ['expElogbeta', 'sstats']\n",
      "INFO - 22:41:19: storing np array 'expElogbeta' to all_subs.model.expElogbeta.npy\n",
      "INFO - 22:41:19: not storing attribute id2word\n",
      "INFO - 22:41:19: not storing attribute state\n",
      "INFO - 22:41:19: not storing attribute dispatcher\n",
      "INFO - 22:41:19: saved all_subs.model\n"
     ]
    }
   ],
   "source": [
    "ldan.save('other.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim as gensimvis\n",
    "import pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 22:50:58: loading LdaModel object from all_subs.model\n",
      "INFO - 22:50:58: loading expElogbeta from all_subs.model.expElogbeta.npy with mmap=None\n",
      "INFO - 22:50:58: setting ignored attribute dispatcher to None\n",
      "INFO - 22:50:58: setting ignored attribute id2word to None\n",
      "INFO - 22:50:58: setting ignored attribute state to None\n",
      "INFO - 22:50:58: loaded all_subs.model\n",
      "INFO - 22:50:58: loading LdaState object from all_subs.model.state\n",
      "INFO - 22:50:58: loading sstats from all_subs.model.state.sstats.npy with mmap=None\n",
      "INFO - 22:50:58: loaded all_subs.model.state\n"
     ]
    }
   ],
   "source": [
    "lda = gensim.models.ldamodel.LdaModel.load('all_subs.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "d = corpora.Dictionary()\n",
    "d.id2token = id2wordn\n",
    "d.token2id = word2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el368311396823344124646914602615\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el368311396823344124646914602615_data = {\"mdsDat\": {\"x\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], \"y\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [54.978489670982626, 5.003158356982625, 5.002295952281743, 5.002293717107572, 5.002293717107572, 5.002293717107572, 5.002293717107572, 5.002293717107572, 5.002293717107572, 5.002293717107572]}, \"tinfo\": {\"Term\": [\"wsb\", \"wsb\", \"wsb\", \"wsb\", \"wsb\", \"wsb\", \"wsb\", \"wsb\", \"wsb\", \"wsb\", \"wsb\"], \"Freq\": [1.0, 0.5497848987579346, 0.05003158375620842, 0.05002295970916748, 0.05002293735742569, 0.05002293735742569, 0.05002293735742569, 0.05002293735742569, 0.05002293735742569, 0.05002293735742569, 0.05002293735742569], \"Total\": [1.0, 1.0000000037252903, 1.0000000037252903, 1.0000000037252903, 1.0000000037252903, 1.0000000037252903, 1.0000000037252903, 1.0000000037252903, 1.0000000037252903, 1.0000000037252903, 1.0000000037252903], \"Category\": [\"Default\", \"Topic1\", \"Topic2\", \"Topic3\", \"Topic4\", \"Topic5\", \"Topic6\", \"Topic7\", \"Topic8\", \"Topic9\", \"Topic10\"], \"logprob\": [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], \"loglift\": [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, \"token.table\": {\"Topic\": [1], \"Freq\": [0.9999999962747097], \"Term\": [\"wsb\"]}, \"R\": 1, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [10, 4, 2, 9, 8, 7, 6, 5, 3, 1]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el368311396823344124646914602615\", ldavis_el368311396823344124646914602615_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el368311396823344124646914602615\", ldavis_el368311396823344124646914602615_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el368311396823344124646914602615\", ldavis_el368311396823344124646914602615_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vis_data = gensimvis.prepare(ldan, corpusn, d)\n",
    "pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 22:49:33: loading Dictionary object from newsgroups.dict\n",
      "INFO - 22:49:33: loaded newsgroups.dict\n",
      "INFO - 22:49:33: loaded corpus index from newsgroups.mm.index\n",
      "INFO - 22:49:33: initializing cython corpus reader from newsgroups.mm\n",
      "INFO - 22:49:33: accepted corpus with 11314 documents, 21098 features, 1051040 non-zero entries\n",
      "INFO - 22:49:33: loading LdaModel object from newsgroups_50.model\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'newsgroups_50.model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-1ec81d4dbfc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'newsgroups.dict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMmCorpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'newsgroups.mm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mlda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mldamodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLdaModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'newsgroups_50.model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1636\u001b[0m         \"\"\"\n\u001b[1;32m   1637\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mmap'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mmap'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1638\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLdaModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1640\u001b[0m         \u001b[0;31m# check if `random_state` attribute has been set after main pickle load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSaveLoad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adapt_by_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loaded %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36munpickle\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m     \"\"\"\n\u001b[0;32m-> 1395\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1396\u001b[0m         \u001b[0;31m# Because of loading from S3 load can't be used (missing readline in smart_open)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, transport_params)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0mtransport_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m     fobj = _shortcut_open(\n\u001b[0m\u001b[1;32m    184\u001b[0m         \u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, ignore_ext, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'errors'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'newsgroups_50.model'"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary.load('newsgroups.dict')\n",
    "corpus = gensim.corpora.MmCorpus('newsgroups.mm')\n",
    "lda = gensim.models.ldamodel.LdaModel.load('newsgroups_50.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'token2id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-695ffc7d5753>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mldan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpusn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pyLDAvis/gensim.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0mSee\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0mopts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_topic_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvis_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pyLDAvis/gensim.py\u001b[0m in \u001b[0;36m_extract_data\u001b[0;34m(topic_model, corpus, dictionary, doc_topic_dists)\u001b[0m\n\u001b[1;32m     24\u001b[0m       \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparse2Corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_csc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m    \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken2id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m    \u001b[0;31m# TODO: add the hyperparam to smooth it out? no beta in online LDA impl.. hmm..\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m    \u001b[0;31m# for now, I'll just make sure we don't ever get zeros...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'token2id'"
     ]
    }
   ],
   "source": [
    "import pyLDAvis.gensim\n",
    "\n",
    "pyLDAvis.gensim.prepare(ldan, corpusn, word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
