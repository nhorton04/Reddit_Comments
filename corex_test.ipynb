{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anchored CorEx: Topic Modeling with Minimal Domain Knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author:** [Ryan J. Gallagher](http://ryanjgallagher.github.io/)  \n",
    "\n",
    "**Last updated:** 07/21/2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook walks through how to use the CorEx topic model code. This includes fitting CorEx to your data, examining the topic model output, outputting results, building a hierarchical topic model, and anchoring words to topics.\n",
    "\n",
    "Details of the CorEx topic model and evaluations against unsupervised and semi-supervised variants of LDA can be found in our TACL paper:\n",
    "\n",
    "Gallagher, Ryan J., Kyle Reing, David Kale, and Greg Ver Steeg. \"[Anchored Correlation Explanation: Topic Modeling with Minimal Domain Knowledge](https://www.transacl.org/ojs/index.php/tacl/article/view/1244).\" *Transactions of the Association for Computational Linguistics (TACL)*, 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as ss\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from corextopic import corextopic as ct\n",
    "from corextopic import vis_topic as vt # jupyter notebooks will complain matplotlib is being loaded twice\n",
    "import re\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the 20 Newsgroups Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to first load data to run the CorEx topic model. We'll use the 20 Newsgroups dataset, which scikit-learn provides functionality to access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ama</th>\n",
       "      <th>askreddit</th>\n",
       "      <th>dankmemes</th>\n",
       "      <th>funny</th>\n",
       "      <th>memes</th>\n",
       "      <th>science</th>\n",
       "      <th>worldnews</th>\n",
       "      <th>wsb</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aa</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000464</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000519</td>\n",
       "      <td>0.0</td>\n",
       "      <td>aa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaa</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>aaa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaaaa</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001659</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>aaaaa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaaaaaaa</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>aaaaaaaa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaaaaaaaaaaa</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.0</td>\n",
       "      <td>aaaaaaaaaaaa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ùôÑùôè</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ùôÑùôè</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ùôèùôÉùôÄ</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ùôèùôÉùôÄ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ùôîùôÄùôé</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ùôîùôÄùôé</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ùöÉùöëùöíùöú</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001659</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ùöÉùöëùöíùöú</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ùöùùöéùöñùöôùöïùöäùöùùöé</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001659</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ùöùùöéùöñùöôùöïùöäùöùùöé</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68993 rows √ó 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ama  askreddit  dankmemes  funny     memes  science  worldnews  \\\n",
       "aa            0.0   0.000464        0.0    0.0  0.000000      0.0   0.000519   \n",
       "aaa           0.0   0.000184        0.0    0.0  0.000000      0.0   0.000000   \n",
       "aaaaa         0.0   0.000000        0.0    0.0  0.001659      0.0   0.000000   \n",
       "aaaaaaaa      0.0   0.000061        0.0    0.0  0.000000      0.0   0.000000   \n",
       "aaaaaaaaaaaa  0.0   0.000052        0.0    0.0  0.000000      0.0   0.000259   \n",
       "...           ...        ...        ...    ...       ...      ...        ...   \n",
       "ùôÑùôè            0.0   0.000061        0.0    0.0  0.000000      0.0   0.000000   \n",
       "ùôèùôÉùôÄ           0.0   0.000123        0.0    0.0  0.000000      0.0   0.000000   \n",
       "ùôîùôÄùôé           0.0   0.000061        0.0    0.0  0.000000      0.0   0.000000   \n",
       "ùöÉùöëùöíùöú          0.0   0.000000        0.0    0.0  0.001659      0.0   0.000000   \n",
       "ùöùùöéùöñùöôùöïùöäùöùùöé      0.0   0.000000        0.0    0.0  0.001659      0.0   0.000000   \n",
       "\n",
       "              wsb         index  \n",
       "aa            0.0            aa  \n",
       "aaa           0.0           aaa  \n",
       "aaaaa         0.0         aaaaa  \n",
       "aaaaaaaa      0.0      aaaaaaaa  \n",
       "aaaaaaaaaaaa  0.0  aaaaaaaaaaaa  \n",
       "...           ...           ...  \n",
       "ùôÑùôè            0.0            ùôÑùôè  \n",
       "ùôèùôÉùôÄ           0.0           ùôèùôÉùôÄ  \n",
       "ùôîùôÄùôé           0.0           ùôîùôÄùôé  \n",
       "ùöÉùöëùöíùöú          0.0          ùöÉùöëùöíùöú  \n",
       "ùöùùöéùöñùöôùöïùöäùöùùöé      0.0      ùöùùöéùöñùöôùöïùöäùöùùöé  \n",
       "\n",
       "[68993 rows x 9 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.read_pickle('./pickles/dtm_idf.pickle')\n",
    "# data_df.transpose()\n",
    "data_df = data_df.transpose()\n",
    "data_df['index'] = data_df.index\n",
    "\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ama</th>\n",
       "      <th>askreddit</th>\n",
       "      <th>dankmemes</th>\n",
       "      <th>funny</th>\n",
       "      <th>memes</th>\n",
       "      <th>science</th>\n",
       "      <th>worldnews</th>\n",
       "      <th>wsb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ab</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001261</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aba</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aback</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000553</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>0.000492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abandon</th>\n",
       "      <td>0.001758</td>\n",
       "      <td>0.000552</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000780</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000695</td>\n",
       "      <td>0.000435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abandoned</th>\n",
       "      <td>0.002797</td>\n",
       "      <td>0.000741</td>\n",
       "      <td>0.000964</td>\n",
       "      <td>0.001241</td>\n",
       "      <td>0.000740</td>\n",
       "      <td>0.000389</td>\n",
       "      <td>0.001105</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ùôÑùôè</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ùôèùôÉùôÄ</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ùôîùôÄùôé</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ùöÉùöëùöíùöú</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001659</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ùöùùöéùöñùöôùöïùöäùöùùöé</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001659</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68933 rows √ó 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                ama  askreddit  dankmemes     funny     memes   science  \\\n",
       "ab         0.000000   0.000311   0.000000  0.000000  0.000000  0.001261   \n",
       "aba        0.000000   0.000123   0.000000  0.000000  0.000000  0.000000   \n",
       "aback      0.000000   0.000078   0.000000  0.000000  0.000000  0.000553   \n",
       "abandon    0.001758   0.000552   0.000000  0.000780  0.000000  0.000000   \n",
       "abandoned  0.002797   0.000741   0.000964  0.001241  0.000740  0.000389   \n",
       "...             ...        ...        ...       ...       ...       ...   \n",
       "ùôÑùôè         0.000000   0.000061   0.000000  0.000000  0.000000  0.000000   \n",
       "ùôèùôÉùôÄ        0.000000   0.000123   0.000000  0.000000  0.000000  0.000000   \n",
       "ùôîùôÄùôé        0.000000   0.000061   0.000000  0.000000  0.000000  0.000000   \n",
       "ùöÉùöëùöíùöú       0.000000   0.000000   0.000000  0.000000  0.001659  0.000000   \n",
       "ùöùùöéùöñùöôùöïùöäùöùùöé   0.000000   0.000000   0.000000  0.000000  0.001659  0.000000   \n",
       "\n",
       "           worldnews       wsb  \n",
       "ab          0.000000  0.000561  \n",
       "aba         0.000000  0.000000  \n",
       "aback       0.000196  0.000492  \n",
       "abandon     0.000695  0.000435  \n",
       "abandoned   0.001105  0.000000  \n",
       "...              ...       ...  \n",
       "ùôÑùôè          0.000000  0.000000  \n",
       "ùôèùôÉùôÄ         0.000000  0.000000  \n",
       "ùôîùôÄùôé         0.000000  0.000000  \n",
       "ùöÉùöëùöíùöú        0.000000  0.000000  \n",
       "ùöùùöéùöñùöôùöïùöäùöùùöé    0.000000  0.000000  \n",
       "\n",
       "[68933 rows x 8 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, row in enumerate(data_df['index']):\n",
    "    candidate = re.match('a{2,}', str(row))\n",
    "    if candidate != None:\n",
    "        data_df.drop(row, inplace=True)\n",
    "    if row == 'ababababba':\n",
    "        data_df.drop(row, inplace=True)\n",
    "    if row == 'wa':\n",
    "        data_df.drop(row, inplace=True)       \n",
    "    if row == 'ha':\n",
    "        data_df.drop(row, inplace=True)    \n",
    "data_df.drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 20 newsgroups data\n",
    "newsgroups = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topic model assumes input is in the form of a doc-word matrix, where rows are documents and columns are binary counts. We'll vectorize the newsgroups data, take the top 20,000 words, and convert it to a sparse matrix to save on memory usage. Note, we use binary count vectors as input to the CorEx topic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 9)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform 20 newsgroup data into a sparse matrix\n",
    "vectorizer = CountVectorizer(stop_words='english', max_features=20000, binary=True)\n",
    "doc_word = vectorizer.fit_transform(data_df)\n",
    "doc_word = ss.csr_matrix(doc_word)\n",
    "\n",
    "doc_word.shape # n_docs x m_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our doc-word matrix is 11,314 documents by 20,000 words. Let's get the words that label the columns. We'll need these for outputting readable topics and later for anchoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ama',\n",
       " 'askreddit',\n",
       " 'dankmemes',\n",
       " 'funny',\n",
       " 'index',\n",
       " 'memes',\n",
       " 'science',\n",
       " 'worldnews',\n",
       " 'wsb']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get words that label the columns (needed to extract readable topics and make anchoring easier)\n",
    "words = list(np.asarray(vectorizer.get_feature_names()))\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll do a final step of preprocessing where we remove all integers from our set of words. This brings is down to 19,038 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 9)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_digit_inds = [ind for ind,word in enumerate(words) if not word.isdigit()]\n",
    "doc_word = doc_word[:,not_digit_inds]\n",
    "words    = [word for ind,word in enumerate(words) if not word.isdigit()]\n",
    "\n",
    "doc_word.shape # n_docs x m_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CorEx Topic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main parameters of the CorEx topic model are:\n",
    "+ **`n_hidden`**: number of topics (\"hidden\" as in \"hidden latent topics\")\n",
    "+ **`words`**: words that label the columns of the doc-word matrix (optional)\n",
    "+ **`docs`**: document labels that label the rows of the doc-word matrix (optional)\n",
    "+ **`max_iter`**: number of iterations to run through the update equations (optional, defaults to 200)\n",
    "+ **`verbose`**:  if `verbose=1`, then CorEx will print the topic TCs with each iteration\n",
    "+ **`seed`**:     random number seed to use for model initialization (optional)\n",
    "\n",
    "We'll train a topic model with 50 topics. (This will take a few minutes.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the CorEx topic model with 50 topics\n",
    "topic_model = ct.Corex(n_hidden=50, words=words, max_iter=200, verbose=False, seed=1)\n",
    "topic_model.fit(doc_word, words=words);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CorEx Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CorEx topic model provides functionality for easily accessing the topics. Let's take a look one of the topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ama', 0.4421466765550526)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print a single topic from CorEx topic model\n",
    "topic_model.get_topics(topic=1, n_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topic words are those with the highest *mutual information* with the topic, rather than those with highest probability within the topic as in LDA. The mutual information with the topic is the number reported in each tuple. Theoretically, mutual information is always positive. If the CorEx output returns a negative mutual information from **`get_topics()`**, then the absolute value of that quantity is the mutual information between the topic and the *absence* of that word.\n",
    "\n",
    "If the column labels have not been specified through **`words`**, then the code will return the column indices for the top words in each topic.\n",
    "\n",
    "We can also retrieve all of the topics at once if we would like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: science\n",
      "1: science\n",
      "2: science\n",
      "3: science\n",
      "4: science\n",
      "5: science\n",
      "6: science\n",
      "7: science\n",
      "8: science\n",
      "9: science\n",
      "10: science\n",
      "11: science\n",
      "12: science\n",
      "13: science\n",
      "14: science\n",
      "15: science\n",
      "16: science\n",
      "17: science\n",
      "18: science\n",
      "19: science\n",
      "20: science\n",
      "21: science\n",
      "22: science\n",
      "23: science\n",
      "24: science\n",
      "25: science\n",
      "26: science\n",
      "27: science\n",
      "28: science\n",
      "29: science\n",
      "30: science\n",
      "31: science\n",
      "32: science\n",
      "33: science\n",
      "34: science\n",
      "35: science\n",
      "36: science\n",
      "37: science\n",
      "38: science\n",
      "39: science\n",
      "40: science\n",
      "41: science\n",
      "42: science\n",
      "43: science\n",
      "44: science\n",
      "45: science\n",
      "46: science\n",
      "47: science\n",
      "48: science\n",
      "49: science\n"
     ]
    }
   ],
   "source": [
    "# Print all topics from the CorEx topic model\n",
    "topics = topic_model.get_topics()\n",
    "for n,topic in enumerate(topics):\n",
    "#     topic_words,_ = zip(*topic)\n",
    "    print('{}: '.format(n) + ','.join(topic_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first topic for the newsgroup data tends to be less coherent than expected because of encodings and other oddities in the newsgroups data.  \n",
    "\n",
    "We can also get the column indices instead of the column labels if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 0.44214544997586624)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topics(topic=5, n_words=10, print_words=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we need to directly access the topic assignments for each word, they can be accessed through **`cluster`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8  9 38 ... 37  0  0]\n",
      "(19038,)\n"
     ]
    }
   ],
   "source": [
    "print(topic_model.clusters)\n",
    "print(topic_model.clusters.shape) # m_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the topic words, the most probable documents per topic can also be easily accessed. Documents are sorted according to log probabilities which is why the highest probability documents have a score of 0 ($e^0 = 1$) and other documents have negative scores (for example, $e^{-0.5} \\approx 0.6$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: 'docs' not provided to CorEx. Returning top docs as lists of row indices\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(3097, 0.0),\n",
       " (2350, 0.0),\n",
       " (105, 0.0),\n",
       " (3864, 0.0),\n",
       " (9396, 0.0),\n",
       " (11229, 0.0),\n",
       " (6440, 0.0),\n",
       " (6437, 0.0),\n",
       " (2284, 0.0),\n",
       " (8445, 0.0)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print a single topic from CorEx topic model\n",
    "topic_model.get_top_docs(topic=0, n_docs=10, sort_by='log_prob')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CorEx is a *discriminative* model, whereas LDA is a *generative* model. This means that while LDA outputs a probability distribution over each document, CorEx instead estimates the probability a document belongs to a topic given that document's words. As a result, the probabilities across topics for a given document do not have to add up to 1. The estimated probabilities of topics for each document can be accessed through **`log_p_y_given_x`** or **`p_y_given_x`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 50)\n"
     ]
    }
   ],
   "source": [
    "print(topic_model.p_y_given_x.shape) # n_docs x k_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use a softmax to make a binary determination of which documents belong to each topic. These softmax labels can be accessed through **`labels`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 50)\n"
     ]
    }
   ],
   "source": [
    "print(topic_model.labels.shape) # n_docs x k_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since CorEx does not prescribe a probability distribution of topics over each document, this means that a document could possibly belong to no topics (all 0's across topics in **`labels`**) or all topics (all 1's across topics in **`labels`**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Correlation and Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall TC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total correlation is the measure which CorEx maximize when constructing the topic model. It can be accessed through **`tc`** and is reported in nats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44.54780845461276"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.tc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model selection:** CorEx starts its algorithm with a random initialization, and so different runs can result in different topic models. One way of finding a better topic model is to restart the CorEx algorithm several times and take the run that has the highest TC value (i.e. the run that produces topics that are most informative about the documents)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic TC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall total correlation is the sum of the total correlation per each topic. These can be accessed through **`tcs`**. For an unsupervised CorEx topic model, the topics are always sorted from high to low according to their TC. For an anchored CorEx topic model, the topics are not sorted, and are outputted such that the anchored topics come first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.tcs.shape # k_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.54780845461276\n",
      "44.54780845461276\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(topic_model.tcs))\n",
    "print(topic_model.tc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Selecting number of topics:** one way to choose the number of topics is to observe the distribution of TCs for each topic to see how much each additional topic contributes to the overall TC. We should keep adding topics until additional topics do not significantly contribute to the overall TC. This is similar to choosing a cutoff eigenvalue when doing topic modeling via LSA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmkAAAFFCAYAAAC393oCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a102a8e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(range(topic_model.tcs.shape[0]), topic_model.tcs, color='#4e79a7', width=0.5)\n",
    "plt.xlabel('Topic', fontsize=16)\n",
    "plt.ylabel('Total Correlation (nats)', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the first topic is much more informative than the other topics. Given that we suspect that this topic is picking up on image encodings (as given by \"dsl\" and \"n3jxp\" in the topic) and other boilerplate text (as given by the high TC and lack of coherence of the rest of the topic), we could consider doing additional investigation and preprocessing to help ensure that the CorEx topic model does not pick up on these patterns which are not insightful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pointwise Document TC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can decompose total correlation further. The topic correlation is the average of the pointwise total correlations for each individual document. The pointwise total correlations can be accessed through **`log_z`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 50)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.log_z.shape # n_docs x k_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.64813418 1.57958569 1.48835238 1.42639341 1.42043661 1.39232446\n",
      " 1.37222362 1.36535367 1.34334261 1.08793513 1.06264005 1.03767991\n",
      " 1.01780633 0.98444346 0.98350034 0.97751588 0.97425573 0.96560217\n",
      " 0.9170467  0.91160502 0.87920818 0.86341492 0.83259909 0.82311403\n",
      " 0.79730235 0.78507652 0.76449939 0.74186846 0.73348486 0.7232203\n",
      " 0.70714498 0.70407292 0.6876558  0.68271949 0.66403509 0.60590956\n",
      " 0.59919508 0.58802044 0.5869777  0.58426131 0.57613847 0.57032326\n",
      " 0.5434933  0.54324576 0.504955   0.48664151 0.28893956 0.27183877\n",
      " 0.24173114 0.21054385]\n",
      "[3.64813418 1.57958569 1.48835238 1.42639341 1.42043661 1.39232446\n",
      " 1.37222362 1.36535367 1.34334261 1.08793513 1.06264005 1.03767991\n",
      " 1.01780633 0.98444346 0.98350034 0.97751588 0.97425573 0.96560217\n",
      " 0.9170467  0.91160502 0.87920818 0.86341492 0.83259909 0.82311403\n",
      " 0.79730235 0.78507652 0.76449939 0.74186846 0.73348486 0.7232203\n",
      " 0.70714498 0.70407292 0.6876558  0.68271949 0.66403509 0.60590956\n",
      " 0.59919508 0.58802044 0.5869777  0.58426131 0.57613847 0.57032326\n",
      " 0.5434933  0.54324576 0.504955   0.48664151 0.28893956 0.27183877\n",
      " 0.24173114 0.21054385]\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(topic_model.log_z, axis=0))\n",
    "print(topic_model.tcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pointwise total correlations in **`log_z`** represent the correlations within an individual document explained by a particular topic. These correlations have been used to measure how \"surprising\" documents are with respect to given topics (see references below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Topic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **`labels`** attribute gives the binary topic expressions for each document and each topic. We can use this output as input to another CorEx topic model to get latent representations of the topics themselves. This yields a hierarchical CorEx topic model. Like the first layer of the topic model, one can determine the number of latent variables to add in higher layers through examination of the topic TCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Some words never appear (or always appear)\n"
     ]
    }
   ],
   "source": [
    "# Train a second layer to the topic model\n",
    "tm_layer2 = ct.Corex(n_hidden=10)\n",
    "tm_layer2.fit(topic_model.labels);\n",
    "\n",
    "# Train a third layer to the topic model\n",
    "tm_layer3 = ct.Corex(n_hidden=1)\n",
    "tm_layer3.fit(tm_layer2.labels);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have `graphviz` installed, then you can output visualizations of the hierarchial topic model to your current working directory. One can also create custom visualizations of the hierarchy by properly making use of the **`labels`** attribute of each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vt.vis_hierarchy([topic_model, tm_layer2, tm_layer3], column_label=words, max_edges=200, prefix='topic-model-example')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anchoring for Semi-Supervised Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anchored CorEx is an extension of CorEx that allows the \"anchoring\" of words to topics. When anchoring a word to a topic, CorEx is trying to maximize the mutual information between that word and the anchored topic. So, anchoring provides a way to guide the topic model towards specific subsets of words that the user would like to explore.  \n",
    "\n",
    "The anchoring mechanism is flexible, and so there are many possibilities of anchoring. We explored the following types of anchoring in our TACL paper:\n",
    "\n",
    "1. Anchoring a single set of words to a single topic. This can help promote a topic that did not naturally emerge when running an unsupervised instance of the CorEx topic model. For example, one might anchor words like \"snow,\" \"cold,\" and \"avalanche\" to a topic if one suspects there should be a snow avalanche topic within a set of disaster relief articles.\n",
    "\n",
    "2. Anchoring single sets of words to multiple topics. This can help find different aspects of a topic that may be discussed in several different contexts. For example, one might anchor \"protest\" to three topics and \"riot\" to three other topics to understand different framings that arise from tweets about political protests.\n",
    "\n",
    "3. Anchoring different sets of words to multiple topics. This can help enforce topic separability if there appear to be chimera topics. For example, one might anchor \"mountain,\" \"Bernese,\" and \"dog\" to one topic and \"mountain,\" \"rocky,\" and \"colorado\" to another topic to help separate topics that merge discussion of Bernese Mountain Dogs and the Rocky Mountains.\n",
    "\n",
    "\n",
    "We'll demonstrate how to anchor words to the the CorEx topic model and how to develop other anchoring strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Anchor one word to the first topic\n",
    "anchor_words = ['nasa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Anchor the word 'nasa' to the first topic\n",
    "anchored_topic_model = ct.Corex(n_hidden=50, seed=2)\n",
    "anchored_topic_model.fit(doc_word, words=words, anchors=anchor_words, anchor_strength=6);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This anchors the single word \"nasa\" to the first topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: nasa,gov,ames,institute,jpl,station,propulsion,jsc,arc,shafer\n"
     ]
    }
   ],
   "source": [
    "topic_words,_ = zip(*anchored_topic_model.get_topics(topic=0))\n",
    "print('0: ' + ','.join(topic_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can anchor multiple groups of words to multiple topics as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Anchor 'nasa' and 'space' to first topic, 'sports' and 'stadium' to second topic, so on...\n",
    "anchor_words = [['nasa', 'space'], ['sports', 'stadium'], ['politics', 'government'], ['love', 'hope']]\n",
    "\n",
    "anchored_topic_model = ct.Corex(n_hidden=50, seed=2)\n",
    "anchored_topic_model.fit(doc_word, words=words, anchors=anchor_words, anchor_strength=6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: space,nasa,orbit,moon,shuttle,launch,gov,earth,lunar,ames\n",
      "1: sports,stadium,april,san,city,los,york,washington,angeles,center\n",
      "2: government,politics,state,rights,law,war,country,military,public,security\n",
      "3: hope,love,helps,relates,virile,tatoos,sustaining,whosoever,weird,allegory\n"
     ]
    }
   ],
   "source": [
    "for n in range(len(anchor_words)):\n",
    "    topic_words,_ = zip(*anchored_topic_model.get_topics(topic=n))\n",
    "    print('{}: '.format(n) + ','.join(topic_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, in the above topic model, topics will no longer be sorted according to descending TC. Instead, the first topic will be the one with \"nasa\" and \"space\" anchored to it, the second topic will be the one with \"sports\" and \"stadium\" anchored to it, and so on.  \n",
    "\n",
    "Observe, the topic with \"love\" and \"hope\" anchored to it is less interpretable than the other three topics. This could be a sign that there is not a good topic around these two words, and one should consider if it is appropriate to anchor around them.\n",
    "\n",
    "We can continue to develop even more involved anchoring strategies. Here we anchor \"nasa\" by itself, as well as in two other topics each with \"politics\" and \"news\" to find different aspects around the word \"nasa\". We also create a fourth anchoring of \"war\" to a topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Anchor with single words and groups of words\n",
    "anchor_words = ['nasa', ['nasa', 'politics'], ['nasa', 'news'], 'war']\n",
    "\n",
    "anchored_topic_model = ct.Corex(n_hidden=50, seed=2)\n",
    "anchored_topic_model.fit(doc_word, words=words, anchors=anchor_words, anchor_strength=6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: nasa,space,orbit,launch,shuttle,moon,earth,lunar,satellite,commercial\n",
      "1: nasa,politics,research,gov,science,scientific,institute,organization,studies,providing\n",
      "2: news,nasa,insisting,edwards,hal,llnl,cso,cfv,nodak,admin\n",
      "3: war,israel,armenians,armenian,israeli,jews,soldiers,military,killed,history\n"
     ]
    }
   ],
   "source": [
    "for n in range(len(anchor_words)):\n",
    "    topic_words,_ = zip(*anchored_topic_model.get_topics(topic=n))\n",
    "    print('{}: '.format(n) + ','.join(topic_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** If you do not specify the column labels through `words`, then you can still anchor by specifying the column indices of the features you wish to anchor on. You may also specify anchors using a mix of strings and indices if desired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choosing anchor strength:** the anchor strength controls how much weight CorEx puts towards maximizing the mutual information between the anchor words and their respective topics. Anchor strength should always be set at a value *greater than* 1, since setting anchor strength between 0 and 1 only recovers the unsupervised CorEx objective. Empirically, setting anchor strength from 1.5-3 seems to nudge the topic model towards the anchor words. Setting anchor strength greater than 5 is strongly enforcing that the CorEx topic model find a topic associated with the anchor words.\n",
    "\n",
    "We encourage users to experiment with the anchor strength and determine what values are best for their needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **`vis_topic`** module provides support for outputting topics and visualizations of the CorEx topic model. The code below creates a results direcory named \"twenty\" in your working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vt.vis_rep(topic_model, column_label=words, prefix='twenty')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our TACL paper details the theory of the CorEx topic model, its sparsity optimization, anchoring via the information bottleneck, comparisons to LDA, and anchoring experiments. The two papers from Greg Ver Steeg and Aram Galstyan develop the CorEx theory in general and provide further motivation and details of the underlying CorEx mechanisms. Hodas et al. demonstrated early CorEx topic model results and investigated an application of pointwise total correlations to quantify \"surprising\" documents.\n",
    "\n",
    "1. [Anchored Correlation Explanation: Topic Modeling with Minimal Domain Knowledge](https://www.transacl.org/ojs/index.php/tacl/article/view/1244), Gallagher et al., TACL 2017.\n",
    "\n",
    "2. [Discovering Structure in High-Dimensional Data Through Correlation Explanation](https://arxiv.org/abs/1406.1222), Ver Steeg and Galstyan, NIPS 2014. \n",
    "\n",
    "3. [Maximally Informative Hierarchical Representions of High-Dimensional Data](https://arxiv.org/abs/1410.7404), Ver Steeg and Galstyan, AISTATS 2015.\n",
    "\n",
    "4. [Disentangling the Lexicons of Disaster Response in Twitter](https://dl.acm.org/citation.cfm?id=2741728), Hodas et al., WWW 2015."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
